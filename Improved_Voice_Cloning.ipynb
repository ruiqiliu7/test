{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.io.wavfile import write\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# Directories and model paths\n",
    "model_dir = \"D:/NEU/NUwork/Voice Cloning Web Application Project/VoiceCloner/media/models\"\n",
    "tacotron2_path = os.path.join(model_dir, \"tacotron2_statedict.pt\")\n",
    "waveglow_path = os.path.join(model_dir, \"waveglow_256channels_universal_v5.pt\")\n",
    "\n",
    "# Verify that the files exist\n",
    "assert os.path.exists(tacotron2_path), f\"Tacotron2 model not found at {tacotron2_path}\"\n",
    "assert os.path.exists(waveglow_path), f\"WaveGlow model not found at {waveglow_path}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10195\\AppData\\Local\\Temp\\ipykernel_18556\\4246807772.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path)['model']\n",
      "d:\\NEU\\NUwork\\Voice Cloning Web Application Project\\myenv\\Lib\\site-packages\\torch\\serialization.py:1189: SourceChangeWarning: source code of class 'torch.nn.modules.conv.ConvTranspose1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "d:\\NEU\\NUwork\\Voice Cloning Web Application Project\\myenv\\Lib\\site-packages\\torch\\serialization.py:1189: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "d:\\NEU\\NUwork\\Voice Cloning Web Application Project\\myenv\\Lib\\site-packages\\torch\\serialization.py:1189: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "def load_waveglow_model(model_path):\n",
    "    model = torch.load(model_path)['model']\n",
    "    model = model.remove_weightnorm(model)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load WaveGlow model\n",
    "waveglow = load_waveglow_model(waveglow_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\10195\\AppData\\Local\\Temp\\ipykernel_18556\\4246807772.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path)['model']\n"
     ]
    }
   ],
   "source": [
    "def load_waveglow_model(model_path):\n",
    "    model = torch.load(model_path)['model']\n",
    "    model = model.remove_weightnorm(model)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Load WaveGlow model\n",
    "waveglow = load_waveglow_model(waveglow_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text, max_len=500, n_mels=80):\n",
    "    sequence = [ord(char) for char in text]\n",
    "    if len(sequence) > max_len:\n",
    "        sequence = sequence[:max_len]\n",
    "    else:\n",
    "        sequence = sequence + [0] * (max_len - len(sequence))\n",
    "    sequence = np.array(sequence)\n",
    "    sequence = sequence.reshape((1, max_len))\n",
    "    sequence_3d = np.zeros((1, n_mels, max_len))\n",
    "    sequence_3d[0, 0, :] = sequence\n",
    "    return sequence_3d\n",
    "\n",
    "def generate_mel_spectrogram(text, model_path):\n",
    "    model = load_model(model_path)\n",
    "    sequence = text_to_sequence(text)\n",
    "    mel_spec = model.predict(sequence)\n",
    "    return mel_spec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mel_spectrogram_to_audio(mel_spec, waveglow):\n",
    "    mel_spec = torch.tensor(mel_spec).unsqueeze(0).to('cuda')\n",
    "    with torch.no_grad():\n",
    "        audio = waveglow.infer(mel_spec)\n",
    "    return audio.cpu().numpy()\n",
    "\n",
    "def generate_audio(text, model_path, waveglow_path):\n",
    "    mel_spec = generate_mel_spectrogram(text, model_path)\n",
    "    waveglow = load_waveglow_model(waveglow_path)\n",
    "    audio = mel_spectrogram_to_audio(mel_spec[0], waveglow)\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 253952, but received input with shape (1, 158720)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 80, 500), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generate audio\u001b[39;00m\n\u001b[0;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, this is a test.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m audio \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwaveglow_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m write(result_path, \u001b[38;5;241m22050\u001b[39m, audio)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerated audio saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[33], line 8\u001b[0m, in \u001b[0;36mgenerate_audio\u001b[1;34m(text, model_path, waveglow_path)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_audio\u001b[39m(text, model_path, waveglow_path):\n\u001b[1;32m----> 8\u001b[0m     mel_spec \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_mel_spectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     waveglow \u001b[38;5;241m=\u001b[39m load_waveglow_model(waveglow_path)\n\u001b[0;32m     10\u001b[0m     audio \u001b[38;5;241m=\u001b[39m mel_spectrogram_to_audio(mel_spec[\u001b[38;5;241m0\u001b[39m], waveglow)\n",
      "Cell \u001b[1;32mIn[32], line 16\u001b[0m, in \u001b[0;36mgenerate_mel_spectrogram\u001b[1;34m(text, model_path)\u001b[0m\n\u001b[0;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m load_model(model_path)\n\u001b[0;32m     15\u001b[0m sequence \u001b[38;5;241m=\u001b[39m text_to_sequence(text)\n\u001b[1;32m---> 16\u001b[0m mel_spec \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mel_spec\n",
      "File \u001b[1;32md:\\NEU\\NUwork\\Voice Cloning Web Application Project\\myenv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\NEU\\NUwork\\Voice Cloning Web Application Project\\myenv\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"dense\" is incompatible with the layer: expected axis -1 of input shape to have value 253952, but received input with shape (1, 158720)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 80, 500), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "model_path = \"D:/NEU/NUwork/Voice Cloning Web Application Project/VoiceCloner/media/models/test_tts_model.keras\"\n",
    "result_path = \"D:/NEU/NUwork/Voice Cloning Web Application Project/VoiceCloner/media/synthesized/test_synthesized_audio.wav\"\n",
    "\n",
    "# Generate audio\n",
    "text = \"Hello, this is a test.\"\n",
    "audio = generate_audio(text, model_path, waveglow_path)\n",
    "write(result_path, 22050, audio)\n",
    "print(f\"Generated audio saved to {result_path}\")\n",
    "\n",
    "# Display audio player\n",
    "display(Audio(audio, rate=22050))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
